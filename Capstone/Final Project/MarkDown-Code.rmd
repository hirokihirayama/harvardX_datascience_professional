---
title: "Start Up Funding"
author: "Hiroki Hirayama"
date: "03/05/2020"
output:
  html_document:
    df_print: paged
---

***'Startup Ecosystem - Funding Patterns'***

Summary : 

The aim of this project is to build a model was to  predict whether a company will survive, get acquired or is no longer operating based on funding data. Startup trends were analysed through drawing up graphs to investigate trends. 

Upon further examination, the data is imbalanced. Regardless, dimensionality reduction algorithm (PCA) and random tree classification was conducted. The results indicated that the nature of the data led to the inability to run a principle component analysis nor random tree classification algorithm - the classification algorithm implemented indicated that it performed no better than just precting a fixed outcome of the majority factor. 

Whilst this is not the outcome hoped for, lessons were drawn from this project - both on how to pick datasets and about the start-up ecosystem. 

***Initial Setup***

The following are the packages used :
Tidyverse,dplyr,ggplot2,stats,ggthemes,stringr,gridExtra,caret,
lubridate,nnet,psych,randomForest

Data : The dataset has been obtained from Kaggle @ https://www.kaggle.com/arindam235/startup-investments-crunchbase
Thank you to Crunchbase (www.crunchbase.com) and Kaggle user 'Andy_M' for kindly providing and making the data set freely available.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=TRUE)
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org") ; library(tidyverse)
if(!require(dplyr)) install.packages("dplyr", repos = "http://cran.us.r-project.org") ; library(dplyr)
if(!require(ggplot2)) install.packages("ggplot2", repos = "http://cran.us.r-project.org") ; library(ggplot2)
if(!require(stats)) install.packages("stats", repos = "http://cran.us.r-project.org") ; library(stats)
if(!require(ggthemes)) install.packages("ggthemes", repos = "http://cran.us.r-project.org") ; library(ggthemes)
if(!require(stringr)) install.packages("stringr", repos = "http://cran.us.r-project.org") ; library(stringr)
if(!require(gridExtra)) install.packages("gridExtra", repos = "http://cran.us.r-project.org") ; library(gridExtra)
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org") ; library(caret)
if(!require(lubridate)) install.packages("lubridate", repos = "http://cran.us.r-project.org") ; library(lubridate)
if(!require(nnet)) install.packages("nnet", repos = "http://cran.us.r-project.org") ; library(nnet)
if(!require(psych)) install.packages("psych", repos = "http://cran.us.r-project.org") ; library(psych)
if(!require(randomForest)) install.packages("randomForest", repos = "http://cran.us.r-project.org") ; library(randomForest)
link <- ("https://raw.githubusercontent.com/hirokihirayama/harvard_capstone/master/Documents/harvardcapstone/Final%20Project/investments_VC.csv")
datasource <- read.csv(link)

```


***Data Pre-processing and Feature Engineering***

```{r echo=TRUE}
head(datasource)
str(datasource)
sum(complete.cases(datasource))/nrow(datasource)
sum(complete.cases(datasource))
describe(datasource)
```

The data imported has a sample size of 54294 with 39 variables.Note that upon initial examination 70.87708% of the data is complete - indicating that there are missing data points.

```{r echo=TRUE, include=FALSE}
datasource1 <- datasource[,-c(1,2,3,4,6,8,9,10,14,15,16,32:39)]
#Removing factors that are redundant.
###Further detected that a few features have "" rather than NA coded into them.
###Forcing NA codes for data with "" entries.
datasource1[datasource1==""] <- NA
describe(datasource1)
```

Note that many duplicated features were found and removed. After removing for this duplicate and non-information rich features for model training, let us recheck the number of complete rows again.

```{r echo=TRUE, include=FALSE}
sum(complete.cases(datasource1))
mean(complete.cases(datasource1))
vcdata <- na.omit(datasource1)
sum(complete.cases(vcdata))
```

After removing all NA data, the number of complete cases fell to 32822.

\newpage

***Feature Engineering***

Upon examination of data, note that the data has to be further proprocessed before ready for fitting.
This includes not only converting the features to the required data types, but also transforming the funding features to the natural log.

```{r echo=TRUE}
vcdata$market <- as.factor(vcdata$market) #Converting to factors.
#Status, Country Code, State Code, City are all already in factors.
#Further investigating "city" feature as it has 4189 levels.
vcdata %>% group_by(city) %>% summarise(n=n()) %>% arrange(desc(n))
#Found features that contain \x in the data.
vcdata$city <- factor(str_replace_all(vcdata$city, "[^[A-Za-z]]", " "))
#Removed all \x in data. Reduced to 4127 levels after removing \x in data.
#Converting to date format.
vcdata$founded_at <- as.Date(vcdata$founded_at, "%Y-%m-%d")
vcdata$first_funding_at <- as.Date(vcdata$first_funding_at, "%Y-%m-%d")
vcdata$last_funding_at <- as.Date(vcdata$last_funding_at, "%Y-%m-%d")

vcdata %>% select(seed,venture,equity_crowdfunding,undisclosed,convertible_note,
  debt_financing,angel,grant,private_equity,post_ipo_equity,post_ipo_debt,
  secondary_market,product_crowdfunding) %>% range()
# Range is 0 to 30,079,503,000. This will be expensive to compute - hence, the
# data will be transformmed with log transformations.

vcdata_funding <- vcdata[,8:20]
#Coding 0s with NAs first as log 0 is infinity.
vcdata_funding[vcdata_funding == 0] <- NA
vcdata_funding <- log(vcdata_funding)
#Recoding NAs with 0.
vcdata_funding[is.na(vcdata_funding)] <- 0
#Checking if transformation was done correctly.
range(vcdata_funding)

#Repackaging dataset.
vcdata[,8:20] <- vcdata_funding
#Removing extra data sets to clean up environment.
rm(vcdata_funding)
```

Further engineering a new feature that computes the number of days between the first and last day of funding.

```{r echo=TRUE}
vcdata <- vcdata%>%mutate(funding_days_gap=last_funding_at-first_funding_at)
#Checking if feature has been implemented correctly.
range(as.numeric(vcdata$funding_days_gap))
#Finding for rows where date gaps are above 5000 days.
sum(vcdata$funding_days_gap>5000)
vcdata_outliers <- vcdata[vcdata$funding_days_gap>5000,]
#Extracting Outlier rows to be further investigated.
vcdata <- vcdata[!(vcdata$funding_days_gap>5000),]
#Removing Outlier rows for east of rebuilding dataset after.
vcdata_outliers$first_funding_at[c(2,5,15,17)] <- as.Date(c("2016-06-01","2012-08-01","2013-07-05","2012-07-24"),"%Y-%m-%d")
vcdata_outliers$last_funding_at[c(2,5,15,17)] <- as.Date(c("2016-07-08","2014-11-19","2019-07-26","2014-11-11"),"%Y-%m-%d")
#Corrected outliers for the actual dates found on CrunchBase.
vcdata_outliers <- vcdata_outliers%>%mutate(funding_days_gap=last_funding_at-first_funding_at)
#Reconstructing Dataset
vcdata <- rbind(vcdata,vcdata_outliers)
#Checking if implemented correctly.
range(vcdata$funding_days_gap)
#Removing redundant status factor level.
vcdata$status <- factor(as.character(vcdata$status))
```

All datasets have been converted to relevant data types.
A new feature "funding_days_gap" which is the days in between the first funding date and last funding date has been computed as well.



####################################################################################

\newpage

***Analysis and Plots***

Understanding the data through visualisations. 

```{r echo=FALSE}
#Pie Chart of Intended Predicted Outcomes

vcdata %>% group_by(status) %>%
  summarise(n=n()) %>%
  mutate(prop=scales::percent(round(n/sum(n),2))) %>%
  ggplot(aes(x="",y=n,fill=status)) +
  geom_bar(width=1,stat="identity") +
  coord_polar("y") + theme_fivethirtyeight() +
  xlab(" ") + ylab(" ") + guides(fill=guide_legend(title="Status"))

#Proportion of startups that are operating are too high - this will definitely cause a problem with the prediction algorithm built.
```

The data is imbalanced. This indicates that there might be some issues anticipated when running dimensionality reduction and prediction. 

\newpage

***Start-up Formation Trends Over Time***

```{r echo=FALSE}
vcdata %>% mutate(founded=substring(founded_at,1,4)) %>%
  count(founded)%>% arrange(-n) %>% head(12) %>%
  ggplot(aes(founded, n)) + geom_col(aes(fill=n)) + coord_flip() +
  theme(legend.position="none") +
  ggtitle("Year Founded") + xlab("") + ylab(" ") + theme_fivethirtyeight() + theme(legend.position="none")
```

The number of start-ups that have been growing steadily with a huge leap in 2006 - 2007.
Note that oddly, there was only a small dip in 2008; indicating that the collapse of the main wall street institutions due to the subprime mortgage crisis did not really impact entrepreneur's mindset that quickly.'

In fact, there was a quick recovery in 2009; indicating that perhaps that after losing their jobs, people are more willing to take risks.
And indeed, after a quick search, University of Missouri did find such patterns as well :
https://munewsarchives.missouri.edu/news-releases/2012/0731-economic-recession-leads-to-increased-entrepreneurship-mu-study-finds/

Recessions drive people to take risks! But obviously, there are confounding factors to take into account here:
- Amazon Web Services was founded in 2016 (allowing for more access to compute power without high initial capital expenditure).
- Launch of iPhone in 2007 - which gave rise to a large rise in the app industry.
- Rise of social media and online entertainment (Facebook in 2004, Youtube in 2005, Twitter in 2006)

Regardless, interesting find!

\newpage 

***Start-up Patterns by Industry***

```{r echo=FALSE}
vcdata %>% mutate(founded=substring(founded_at,1,4)) %>% filter(market!="") %>%
  group_by(market) %>% summarize(n=n())%>% arrange(-n) %>% head(10) %>%
  ggplot(aes(reorder(market, n), n)) + geom_col(aes(fill=n)) + coord_flip() +
  theme(legend.position="none") +
  ggtitle("Top 10 Industries") + xlab("Market") + ylab("Count") +
  theme_fivethirtyeight()
```

As expected, the software industry has the highest number of start-ups; with biotechnology right behind it.There seem to be a large gap in between biotechnology and mobile; the number of mobile is roughly half of biotechnology.

\newpage

***Start-up Patterns by Industry***

```{r echo=FALSE}
vcdata %>% mutate(founded=substring(founded_at,1,4)) %>%
  filter(market %in% c(' Software ',' Biotechnology ',' Mobile ', ' Curated Web ',
  ' Enterprise Software ', ' Health Care ', ' E-Commerce ',' Hardware + Software ',' Health and Wellness ', ' Advertising ')) %>%
  filter(founded>=2004) %>% group_by(market,founded) %>% summarize(n=n())%>% arrange(-n) %>%
  ggplot(aes(x=founded,y=n, fill=market)) + geom_col(position="fill") +
  theme(legend.position="none") +
  ggtitle("Industry Trends from 2004 to 2014") +
  theme_fivethirtyeight() + theme(axis.text.x = element_text(angle = 90, hjust = 1)) + guides(fill=guide_legend(title="Market"))
```

The trends show that the patterns over the past 10 years tend to be relatively stable - with the exception of :

- Increment of software startups in 2014. This is due to post 2008 crisis funding increment (particularly in venture capital).

- Decrement of share of biotechnology startups starting from 2004 until 2014. Whilst it is not clear why, personal research indicate
that hurdles required to bypass in order to make a successful marketable product is difficult - and the initial investment required in biotechnology companies are high.

\newpage 

***Location***

```{r echo=FALSE}
vcdata %>% count(city)%>% arrange(-n) %>% head(10) %>%
 ggplot(aes(reorder(city, n), n)) + geom_col(aes(fill=city),show.legend=FALSE) + coord_flip() +
 theme(legend.position="none") +
 ggtitle("Top 10 Cities") + xlab(" ") + ylab(" ") + theme_fivethirtyeight()
```

As expected, the highest number of startups in the world are all based in the United States.
Let's find out which cities are reputable for start-ups outside of the United States.

\newpage 

***Distribution of Funding***


```{r echo=FALSE}

seed <- vcdata %>% filter(seed>0) %>% ggplot(aes(x=seed,fill=status)) + geom_histogram(aes(fill=status),bins=70) + xlab(" ") + ylab(" ") + theme_fivethirtyeight() + ggtitle("Seed Funding") + theme(legend.text=element_text(size=8), legend.key.size = unit(0.25,"line"), plot.title = element_text(size = 10, face = "bold")) +theme(legend.title = element_text(size = 8)) + theme(legend.position="none") + theme(axis.text.x = element_text(size=6),axis.text.y=element_text(size=6))

equity_crowdfunding <- vcdata %>% filter(equity_crowdfunding>0) %>% ggplot(aes(x=equity_crowdfunding,fill=status)) + geom_histogram(aes(fill=status),bins=70) + xlab(" ") + ylab(" ") + theme_fivethirtyeight() + ggtitle("Equity Crowd Funding") + theme(plot.title = element_text(size = 10, face = "bold")) +
theme(legend.position="none") + theme(axis.text.x = element_text(size=6),axis.text.y=element_text(size=6)) +  scale_fill_manual(values=c("#f8766d","#639cfb","black"))

convertiblenote <- vcdata %>% filter(convertible_note>0) %>% ggplot(aes(x=convertible_note,fill=status)) + geom_histogram(aes(fill=status),bins=70) + xlab(" ") + ylab(" ") + theme_fivethirtyeight() + ggtitle("Convertible Notes Funding ") + theme(legend.text=element_text(size=8), legend.key.size = unit(0.25,"line"), plot.title = element_text(size = 10, face = "bold")) +theme(legend.title = element_text(size = 8)) + theme(legend.position="none") + theme(axis.text.x = element_text(size=6),axis.text.y=element_text(size=6))

debtfinancing <- vcdata %>% filter(debt_financing>0) %>% ggplot(aes(x=debt_financing,fill=status)) + geom_histogram(aes(fill=status),bins=70) + xlab(" ") + ylab(" ") + theme_fivethirtyeight() + ggtitle("Debt Financing") +
theme(plot.title = element_text(size = 10, face = "bold")) +
theme(legend.position="none")+ theme(axis.text.x = element_text(size=6),axis.text.y=element_text(size=6))

angel <- vcdata %>% filter(angel>0) %>% ggplot(aes(x=angel,fill=status)) + geom_histogram(aes(fill=status),bins=70) + xlab(" ") + ylab(" ") + theme_fivethirtyeight() + ggtitle("Angel Funding") +
theme(plot.title = element_text(size = 10, face = "bold")) +
theme(legend.position="none")+ theme(axis.text.x = element_text(size=6),axis.text.y=element_text(size=6))

grant <- vcdata %>% filter(grant>0) %>% ggplot(aes(x=grant,fill=status)) + geom_histogram(aes(fill=status),bins=70) + xlab(" ") + ylab(" ") + theme_fivethirtyeight() + ggtitle("Grant Funding ") +
theme(plot.title = element_text(size = 10, face = "bold")) +
theme(legend.position="none")+ theme(axis.text.x = element_text(size=6),axis.text.y=element_text(size=6))

private_equity <- vcdata %>% filter(private_equity>0) %>% ggplot(aes(x=private_equity,fill=status)) + geom_histogram(aes(fill=status),bins=70) + xlab(" ") + ylab(" ") + theme_fivethirtyeight() + ggtitle("Private Equity") +
theme(plot.title = element_text(size = 10, face = "bold")) +
theme(legend.position="none")+ theme(axis.text.x = element_text(size=6),axis.text.y=element_text(size=6))

post_ipo_equity<- vcdata %>% filter(post_ipo_equity>0) %>% ggplot(aes(x=post_ipo_equity,fill=status)) + geom_histogram(aes(fill=status),bins=70) + xlab(" ") + ylab(" ") + theme_fivethirtyeight() + ggtitle("Post IPO Equity") + theme(plot.title = element_text(size = 10, face = "bold")) +
theme(legend.position="none")+ theme(axis.text.x = element_text(size=6),axis.text.y=element_text(size=6))

undisclosed <- vcdata %>% filter(undisclosed>0) %>% ggplot(aes(x=undisclosed,fill=status)) + geom_histogram(aes(fill=status),bins=70)  + xlab(" ") + ylab(" ") + theme_fivethirtyeight() + ggtitle("Undisclosed Funding") + theme(plot.title = element_text(size = 10, face = "bold"))  +
theme(legend.position="none") + theme(axis.text.x = element_text(size=6),axis.text.y=element_text(size=6))

post_ipo_debt<- vcdata %>% filter(post_ipo_debt>0) %>% ggplot(aes(x=post_ipo_debt,fill=status)) + geom_histogram(aes(fill=status),bins=70) + xlab(" ") + ylab(" ") + theme_fivethirtyeight() + ggtitle("Post IPO Debt") + theme(plot.title = element_text(size = 10, face = "bold")) +
theme(legend.position="none")+ theme(axis.text.x = element_text(size=6),axis.text.y=element_text(size=6)) +  scale_fill_manual(values=c("#f8766d","#639cfb","black"))

product_crowdfunding <- vcdata %>% filter(product_crowdfunding>0) %>% ggplot(aes(x=product_crowdfunding,fill=status)) + geom_histogram(aes(fill=status),bins=70) + xlab(" ") + ylab(" ") + theme_fivethirtyeight() + ggtitle("Product Crowdfunding") +
theme(plot.title = element_text(size = 10, face = "bold")) +
theme(legend.position="none") + scale_fill_manual(values=c("#f8766d","#639cfb","black"))+ theme(axis.text.x = element_text(size=6),axis.text.y=element_text(size=6))

secondary_market <- vcdata %>% filter(secondary_market>0) %>% ggplot(aes(x=secondary_market,fill=status)) + geom_histogram(aes(fill=status),bins=70) + xlab(" ") + ylab(" ") + theme_fivethirtyeight() + ggtitle("Secondary Market") +
theme(plot.title = element_text(size = 10, face = "bold")) +
theme(legend.position="none")+ theme(axis.text.x = element_text(size=6),axis.text.y=element_text(size=6))

grid.arrange(secondary_market,post_ipo_debt,post_ipo_equity,product_crowdfunding,equity_crowdfunding,convertiblenote,grant,undisclosed,private_equity,angel,debtfinancing, seed,ncol=3,nrow=4)

#Plotting Undisclosed Funding
vcdata %>% filter(venture>0) %>% ggplot(aes(x=venture,fill=status)) + geom_histogram(aes(fill=status),bins=30) + xlab(" ") + ylab(" ") + theme_fivethirtyeight() + ggtitle("Venture Capital") + theme(plot.title = element_text(size = 10, face = "bold"))  
```



Note that the X axis has been log transformed.

As seen from the graph above, main sources are relatively normally distributed.
However, the funding that is low in frequency (i.e., secondary market, post IPO debt) are not - 
this is likely due to the low numbers of funding given out in each of these funding categories. 

Let's analyse this further:

- Seed funding - This distribution displays a negative skew; indicating that most seed funding tend to be on the lower end. This is to be expected as seed funding is given for companies that are.

- Venture Capital - The distribution is slightly negative skewed. The mean is higher than seed funding's mean as well - this is to be expected as venture capital funding is normally for companies that are in the later development stages of a startup.

Note that the frequencies of both seed funding and venture capital is high - relative to other sources of funding.

The distribution of undisclosed funding is relatively normal - indicating that this category most likely contains data from all forms of funding aggregated together. 

\newpage 

***Funding Patterns***

```{r echo=FALSE}
#Funding Days Gap
gap <-vcdata %>% filter(as.numeric(funding_days_gap)>0) %>% ggplot(aes(as.numeric(funding_days_gap))) +
  geom_histogram(bins=20)  +
  ggtitle("Gap in Between Last and First Funding Date (Days)") + xlab(" ") + ylab(" ") + theme_fivethirtyeight() + theme(plot.title = element_text(size = 6, face = "bold"))

#Plotting Funding Rounds Distribution
rounds <- vcdata %>% filter(post_ipo_debt>0) %>%ggplot(aes(x=funding_rounds)) + geom_histogram(bins=30) + xlab(" ") + ylab(" ") + theme_fivethirtyeight() + ggtitle("Funding Rounds Obtained") + theme(plot.title = element_text(size = 6, face = "bold"))

grid.arrange(gap,rounds,ncol=2,nrow=1)
```


The mode of funding distributions is 1 funding round - indicating that most start-ups only go through one funding round. 

Funding after the first round decreases significantly - most startups don't get through to the 2nd round of funding.

***Sub-Conclusion***

A lot can be learnt about the start-up funding scene just from these graphs. And, as observed before, the data is imbalanced, hence, it is very likely that algorithms implemented will be ineffective.

Regardless, let's get on with the algorithms.

\newpage 

***Algorithm Implementation 1 - Dimensionality Reduction : PCA***

Principal Component Analysis was fitted onto the data.
Before doing that, all categorical data was converted into dummy coding.

```{r echo=TRUE}
#Removing non.numeric vectors
vcdataforpc <- vcdata[,-c(1,2,3,5,6,7)]
vcdataforpc$funding_days_gap <- as.numeric(vcdataforpc$funding_days_gap)
vcdata.pr <- prcomp(vcdataforpc,center=TRUE,scale=TRUE)
summary(vcdata.pr)
#Cut off point decided for Eigenvalue = 1
screeplot(vcdata.pr, type = "l", npcs = 20, 
          main = "Screeplot of Principal Component Eigenvalues")
abline(h = 1, col="blue", lty=5)
legend("topright", legend=c("Eigenvalue of 1"),
       col=c("blue"), lty=5, cex=1.00)
#Further checking if data is suitable for pca.
cumpro <- cumsum(vcdata.pr$sdev^2 / sum(vcdata.pr$sdev^2))
plot(cumpro[0:15], xlab = "PC #", ylab = "Amount of explained variance", 
     main = "Cumulative variance plot")
#Not suitable for running PCA.
```

The results indicated that the data is not suitable for principle component analysis. This can be seen from how none of the factors captured more variance, in comparison to the others - leading to an almost linear graph in the "Cumulative variance plot". 

This is to be expected as the data is structured in a manner where most features are either encoded with the relevant funding amount or coded as '0' for any specific funding category. For instance, a respective startup could have a large amount in VC funding; but nil for everything else.

\newpage 

***Algorithm Implementation 2 : Random Tree***

Let us try implementing classification algorithms to see how prediction performance would be like in this dataset.

Before that, let us split the data set to train and test set.
```{r echo=TRUE, include=FALSE }
set.seed(1,sample.kind="Rounding")
test_index <- createDataPartition(vcdata$status,times=1,p=0.8,list=FALSE)
train <- vcdata[test_index,]
test <- vcdata[-test_index,]
set.seed(1,sample.kind="Rounding")
```



```{r echo=TRUE}
#Removing categorical predictors > 53 categories as Rtree cannot handle more than 53 categories.
train_forest <- train[,-c(1,3)]
train_rf <- randomForest(status ~ ., data=train_forest)
train_rf
```

The initial random tree forest ran indicate that the algorithm cannot accurately predict whether a start up will be closed or acquired;
this is expected as the data is imbalanced - there are more operating start-ups than ones that are acquired vs closed.

\newpage 

Let's tune the parameters just to see if a better outcome could be achieved.

```{r echo=TRUE}
#Finding best mtry tuning. 
res <- tuneRF(x = subset(train_forest, select=-status),
              y = train_forest$status,
              ntreeTry = 500)
#Lowest mtry = 2 
#Finding best node size tuning. 
nodesize <- seq(1, 10)
oob_err <- c()
for (i in 1:length(nodesize)) {
    model <- randomForest(formula = status ~ .,
                          data = train_forest,
                          nodesize = nodesize[i],mtry=2)
    oob_err[i] <- model$err.rate[nrow(model$err.rate), "OOB"]
}
which.min(oob_err)

#Optimal Random Forest Model : Nodesize 4, mtry = 2

finalrtree <- randomForest(status ~ ., data=train_forest,mtry=2,nodesize=4)
```

Hence, the final random forest model trained has paramaters mtry=2 and nodesize = 1.
Let's further evaluate the performance of this algorithm on the train set.

```{r echo=TRUE}
test_forest<- test[,-c(1,3)]
#Prediction by assuming that all outcomes are "operating"
guess <- mean(test_forest$status=="operating")
#Accuracy of just predicing that all startups are operating is 86.4%

#Random Forest Tree
random <- confusionMatrix(predict(finalrtree, test_forest), test_forest$status)$overall["Accuracy"]
#Rtree's final accuracy is 86.5%.

print(random-guess)
```

The results indicate that the accuracy of the randomForest model is almost alike predicing all companies as still operating.
The accuracy of prediction of randomForest performs marginally better by 00.015% - which is approximately equivalent to a NULL improvement in accuracy. 

This indicates that there is no difference in performance here. This is to be expected due to the nature of the data.

\newpage 

***Conclusion : Results and Limitations ***

The PCA and Random Tree Algorithm ran indicate that the data is not suitable for dimensionality reduction, nor for prediction. This is most likely due to the imbalanced nature of the data. 

However, from the plots, a few conclusions about the start-up industry can be made : 

- Most start-ups are still operating up until 2013; in fact, merely predicting that all start-ups are operating allows gives us an accuracy that is equivalent to a random tree algorithm prediction ran. 
- VC funding and Seed funding are the most widespread - as seen from the plots done. Not many companies obtain post IPO financing. 
- Most start-ups are based in the United States; this is unsurprising given the notion of "Silicon Valley"
- Economic recessions lead to a spike in entrepreneurship and the number of biotechnology startups have been decreasin - perhaps we will see a spike again post COVID-19 era? 

Here are the lessons drawn from this project : 

- Always check for data balance before even thinking of cleaning up/analysing the data. 
- If data is imbalance, the algorithm will most likely performing as well as an educated guess of predicting that the test outcome is the majority group in the unbalanced data. 

Thank you for reading through this. I hope you enjoyed the graphs! 

***End***

